{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jared-r-chen/cs257/blob/main/notebooks/table5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oFUk2MstQ2s"
      },
      "source": [
        "### Table 5 generator\n",
        "\n",
        "Creates the data for table 5 in the appendix. Based off code in baseline.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-uVob5htQ2u"
      },
      "source": [
        "## Colab setup\n",
        "\n",
        "This section is only pertinent if the notebook is run in Colab and not on a local machine. If you're using colab, make sure to run below code to clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDOFCH5ztQ2u"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mtzig/NLP_CTF.git\n",
        "%cd /content/NLP_CTF/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhG8z5-htQ2v"
      },
      "source": [
        "Download Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY5v9F-gtQ2v"
      },
      "outputs": [],
      "source": [
        "%cd /content/NLP_CTF/data\n",
        "!wget -O GoogleNews-vectors-negative300.bin  'https://www.dropbox.com/s/mlg71vsawice3xd/GoogleNews-vectors-negative300.bin?dl=1'\n",
        "%cd ./civil_comments\n",
        "!wget -O civil_comments.csv 'https://www.dropbox.com/s/xv8zkmcmg74n0ak/civil_comments.csv?dl=1'\n",
        "%cd ..\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElEqk_HttQ2v"
      },
      "source": [
        "Colab does not have the Python library `transformers` (which I use in below code) automatically installed, so we meed to manually install when we start up instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbE1ubI3tQ2w"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-43gTItQ2w"
      },
      "source": [
        "## Notebook Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KevR8TjbtQ2w"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf00a88ltQ2x"
      },
      "outputs": [],
      "source": [
        "# %cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whFxUVHwtQ2x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from process_data import get_jigsaw_datasets, init_embed_lookup, get_ctf_datasets, get_CivilComments_Datasets\n",
        "from models import CNNClassifier\n",
        "from train_eval import train, evaluate, CTF, get_pred\n",
        "from torch.utils.data import DataLoader\n",
        "from loss import CLP_loss, ERM_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkAM4ybBtQ2x"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('Using GPU')\n",
        "    DEVICE = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "    # macbooks can use metal if the right version of pytorch is installed\n",
        "    print('Using Metal')\n",
        "    DEVICE = torch.device('mps')\n",
        "else:\n",
        "    print('Using cpu')\n",
        "    DEVICE = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMa99wNRtQ2x"
      },
      "source": [
        "## Data Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHzN5q3tQ2x"
      },
      "source": [
        "Pytorch requires its datasets to be ascessible following the [datasets api](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).\n",
        "\n",
        "Below I wrote a simple function to load in the [Jigsaw Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) that the paper [Counterfactual Fairness in\n",
        "Text Classification through Robustness](https://dl.acm.org/doi/pdf/10.1145/3306618.3317950) used to train its toxicity classifier.\n",
        "\n",
        "I use only a very small subset of the available data here for demonstration purposes. Specificaly 256 comments (128 toxic and 128 nontoxic) sampled randomly for the train set and test set respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p0uEQUmtQ2y"
      },
      "outputs": [],
      "source": [
        "embed_lookup = init_embed_lookup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CpMJONbtQ2y"
      },
      "outputs": [],
      "source": [
        "train_data = get_jigsaw_datasets(device=DEVICE, data_type='baseline', embed_lookup=embed_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFFxwP60tQ2y"
      },
      "source": [
        "PyTorch models receive data for training and inference through a dataloader. A dataloader samples from a dataset and returns a batch of samples each time it is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quyu6qHatQ2y"
      },
      "outputs": [],
      "source": [
        "train_loader =  torch.utils.data.DataLoader(train_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOAIneNUtQ2y"
      },
      "source": [
        "## Model and Training Stuff Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs7m0jH2tQ2y"
      },
      "outputs": [],
      "source": [
        "pretrained_embed = torch.from_numpy(embed_lookup.vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1zAq8fytQ2y"
      },
      "outputs": [],
      "source": [
        "model = CNNClassifier(pretrained_embed,device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ke3zcn4tQ2y"
      },
      "source": [
        "An epoch is the number of times you go through your datase during training. That is you have trained for 1 epoch when you have seen every sample in your training dataset once.<br>\n",
        "The loss function is the training objective we want our model to minimize.<br>\n",
        "The optimizer is used at every time step i.e. everyime we compute the loss and its gradient. It is used to update the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfe2aadztQ2z"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "loss_fn = ERM_loss(torch.nn.CrossEntropyLoss())\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF6DMFNmtQ2z"
      },
      "source": [
        "## Training and Evaulation Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C3gSjTutQ2z"
      },
      "source": [
        "For traing, we train for 10 epochs. <br>\n",
        "In general, you should (or more specifically are required to) train and evaluate using different datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBXZ8bbLtQ2z"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    train(train_loader, model, loss_fn, optimizer, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKVD6hFKtQ2z"
      },
      "outputs": [],
      "source": [
        "get_pred('f', model, embed_lookup=embed_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-66DGE-tQ2z"
      },
      "source": [
        "## setup train eval for blindness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RPyxS_OtQ2z"
      },
      "outputs": [],
      "source": [
        "train_data = get_jigsaw_datasets(device=DEVICE, data_type='blindness', embed_lookup=embed_lookup)\n",
        "train_loader =  torch.utils.data.DataLoader(train_data, batch_size=64)\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "loss_fn = ERM_loss(torch.nn.CrossEntropyLoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSLjj6cLtQ2z"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    train(train_loader, model, loss_fn, optimizer, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXjzq0edtQ2z"
      },
      "outputs": [],
      "source": [
        "get_pred('f', model, embed_lookup=embed_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h-M7daMtQ2z"
      },
      "source": [
        "## Setup train eval for CTF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWRaEAuztQ2z"
      },
      "outputs": [],
      "source": [
        "train_data, A = get_jigsaw_datasets(device=DEVICE, data_type='CLP', embed_lookup=embed_lookup)\n",
        "train_loader =  torch.utils.data.DataLoader(train_data, batch_size=64)\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "loss_fn = CLP_loss(torch.nn.CrossEntropyLoss(), A, lmbda=float(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omSW7usKtQ2z"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    train(train_loader, model, loss_fn, optimizer, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo96UFegtQ2z"
      },
      "outputs": [],
      "source": [
        "get_pred('I am gay', model, embed_lookup=embed_lookup)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "642a120c79627041cabaf7004696707442c14ddd51cc7fda1a5975e0f351036f"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}